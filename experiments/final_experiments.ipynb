{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-11T09:08:45.989810Z",
     "start_time": "2024-07-11T09:08:45.987424Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append(\"../backend\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:08:48.826629Z",
     "start_time": "2024-07-11T09:08:48.812204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(filename=\"local.env\"))"
   ],
   "id": "f03d58300b521baf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:08:50.710561Z",
     "start_time": "2024-07-11T09:08:49.659573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import QAGenerationChain\n",
    "from config import config"
   ],
   "id": "151452cb4990a8c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to database:  localhost 5432 ai4esg\n",
      "log path:  ai4esg.log\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:08:51.690233Z",
     "start_time": "2024-07-11T09:08:51.685716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, value in config.__dict__.items():\n",
    "    print(f'{key}={value}')"
   ],
   "id": "22508c78248ed35c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai_api_key=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\n",
      "azure_openai_key=5c290765f18449559375b6108879be23\n",
      "api_endpoint=http://127.0.0.1:8000/v1\n",
      "api_version=2023-05-15\n",
      "azure_gpt4_deployment_name=AI4ESGGPT4\n",
      "embedding_model=text-embedding-ada-002\n",
      "chunk_size=1024\n",
      "overlap_size=0\n",
      "max_content=3\n",
      "prompt_size=6000\n",
      "add_title=False\n",
      "answer_do_not_know=False\n",
      "temperature=0.0001\n",
      "use_hybrid=True\n",
      "use_reranking=True\n",
      "hybrid_fusion=reciprocal_rank\n",
      "db_user=postgres\n",
      "db_pass=postgres\n",
      "db_host=localhost\n",
      "db_port=5432\n",
      "db_name=ai4esg\n",
      "app_path=http://localhost:8501\n",
      "log_path=ai4esg.log\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:08:55.132785Z",
     "start_time": "2024-07-11T09:08:54.494672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = config.azure_openai_key\n",
    "openai.api_base = config.api_endpoint\n",
    "openai.api_version = \"2023-05-15\""
   ],
   "id": "558b573e87cdfc0f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:08:55.947369Z",
     "start_time": "2024-07-11T09:08:55.679011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from backend.modules.document.utils.token_utils import TokenStats\n",
    "\n",
    "token_stats = TokenStats()"
   ],
   "id": "c4e424a42bbee55a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:08:58.274857Z",
     "start_time": "2024-07-11T09:08:56.628041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "\n",
    "chatOpenAI = AzureChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    deployment_name=config.azure_gpt4_deployment_name,\n",
    "    openai_api_base=openai.api_base,\n",
    "    openai_api_version=openai.api_version,\n",
    "    openai_api_key=openai.api_key\n",
    ")"
   ],
   "id": "c8970618d67dd1f7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\onan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:167: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from http://127.0.0.1:8000/v1 to http://127.0.0.1:8000/v1/openai.\n",
      "  warnings.warn(\n",
      "C:\\Users\\onan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:174: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\onan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:182: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating http://127.0.0.1:8000/v1 to http://127.0.0.1:8000/v1/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:08:59.428625Z",
     "start_time": "2024-07-11T09:08:59.425504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_prompt_multi = '''You are a legal expert and a teacher coming up with questions to ask on a quiz about ESG regulations. \\\n",
    "\\nGiven the following legal document, please generate {num_questions} questions and corresponding answers based on that document. \\\n",
    "\\nGenerate only short questions without compound sentences. Generate a variety of questions that cover different aspects about the document. \\\n",
    "\\nImportant: In the questions use a variety of question words such as: why, how, where, when, who, which. \\\n",
    "\\n Here are some topic ideas for the questions: \n",
    "    1. requirements for compliance\n",
    "    2. penalties for non-compliance\n",
    "    3. to whom the regulations apply\n",
    "    4. criteria for applicability of this regulation\n",
    "\\nOnly if the document does not contain information about those topics, include your own. \\\n",
    "\\nMake sure they are complete questions, and that they can be answered by extracting excerpts from the document. \\\n",
    "\\nEach answer must be composed of an exactly copied excerpt from the document.\\\n",
    "\\n\\nThese questions and answers should be detailed and be based explicitly on information in the document. \\\n",
    "\\n{format_instructions} \\\n",
    "\\nBegin! \\\n",
    "\\n\\n<Begin Document>\\n{doc}\\n<End Document>'''"
   ],
   "id": "ad40e72cb8f35e6c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:01.425282Z",
     "start_time": "2024-07-11T09:09:01.420859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    text: str = Field(description=\"Answer text\")\n",
    "    \n",
    "\n",
    "class QA(BaseModel):\n",
    "    question: str = Field(description=\"Question text\")\n",
    "    answers: list[Answer] = Field(description=\"List of answers\")\n",
    "\n",
    "class QAS(BaseModel):\n",
    "    qas: list[QA] = Field(description=\"List of questions and answers\")"
   ],
   "id": "3c0ec601c2592789",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:03.437342Z",
     "start_time": "2024-07-11T09:09:03.434229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_gen_chain = QAGenerateChain.from_string(chatOpenAI, qa_prompt_multi)\n",
    "qa_gen_chain.verbose = True\n",
    "qa_gen_chain.output_key = \"response\""
   ],
   "id": "bb3b3bc91c093bbd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:05.311185Z",
     "start_time": "2024-07-11T09:09:05.301513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "output_parser = SimpleJsonOutputParser(pydantic_object=QAS)\n",
    "\n",
    "qa_gen_chain.output_parser = output_parser"
   ],
   "id": "fc4846aefdfd1be0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:06.546006Z",
     "start_time": "2024-07-11T09:09:06.541750Z"
    }
   },
   "cell_type": "code",
   "source": "qa_gen_chain.output_parser",
   "id": "b85f3bb3f9f5709a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JsonOutputParser(pydantic_object=<class '__main__.QAS'>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:07.452621Z",
     "start_time": "2024-07-11T09:09:07.448403Z"
    }
   },
   "cell_type": "code",
   "source": "output_parser.get_format_instructions()",
   "id": "f7ed3ff0890ebcb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"qas\": {\"title\": \"Qas\", \"description\": \"List of questions and answers\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/QA\"}}}, \"required\": [\"qas\"], \"definitions\": {\"Answer\": {\"title\": \"Answer\", \"type\": \"object\", \"properties\": {\"text\": {\"title\": \"Text\", \"description\": \"Answer text\", \"type\": \"string\"}}, \"required\": [\"text\"]}, \"QA\": {\"title\": \"QA\", \"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"description\": \"Question text\", \"type\": \"string\"}, \"answers\": {\"title\": \"Answers\", \"description\": \"List of answers\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Answer\"}}}, \"required\": [\"question\", \"answers\"]}}}\\n```'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:09.563877Z",
     "start_time": "2024-07-11T09:09:09.561137Z"
    }
   },
   "cell_type": "code",
   "source": "format_instructions = output_parser.get_format_instructions()",
   "id": "9f14df4d940e2e42",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:11.525099Z",
     "start_time": "2024-07-11T09:09:11.522403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key in qa_gen_chain.prompt.__dict__:\n",
    "    print(f'{key}')"
   ],
   "id": "61d0528067cc9590",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "input_variables\n",
      "input_types\n",
      "output_parser\n",
      "partial_variables\n",
      "metadata\n",
      "tags\n",
      "template\n",
      "template_format\n",
      "validate_template\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:13.389722Z",
     "start_time": "2024-07-11T09:09:13.385956Z"
    }
   },
   "cell_type": "code",
   "source": "qa_gen_chain.prompt",
   "id": "daa9d11548579d6f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['doc', 'format_instructions', 'num_questions'], template='You are a legal expert and a teacher coming up with questions to ask on a quiz about ESG regulations. \\nGiven the following legal document, please generate {num_questions} questions and corresponding answers based on that document. \\nGenerate only short questions without compound sentences. Generate a variety of questions that cover different aspects about the document. \\nImportant: In the questions use a variety of question words such as: why, how, where, when, who, which. \\n Here are some topic ideas for the questions: \\n    1. requirements for compliance\\n    2. penalties for non-compliance\\n    3. to whom the regulations apply\\n    4. criteria for applicability of this regulation\\n\\nOnly if the document does not contain information about those topics, include your own. \\nMake sure they are complete questions, and that they can be answered by extracting excerpts from the document. \\nEach answer must be composed of an exactly copied excerpt from the document.\\n\\nThese questions and answers should be detailed and be based explicitly on information in the document. \\n{format_instructions} \\nBegin! \\n\\n<Begin Document>\\n{doc}\\n<End Document>')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:19.161043Z",
     "start_time": "2024-07-11T09:09:19.158791Z"
    }
   },
   "cell_type": "code",
   "source": "num_questions = 2",
   "id": "d50c372731687158",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:09:34.388516Z",
     "start_time": "2024-07-11T09:09:21.819743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Get document for chain here\n",
    "from modules.document.service import get_all_titles\n",
    "titles = get_all_titles()\n",
    "excluded = [\n",
    "    \"Gesetz über das Inverkehrbringen, die Rücknahme und die umweltverträgliche Entsorgung von Batterien und Akkumulatoren (Batteriegesetz - BattG)\",\n",
    "    \"REGULATION (EU) 2019/2088 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 27 November 2019 on sustainability‐related disclosures in the financial services sector (Text with EEA relevance)\",\n",
    "    \"Indian Standard GUIDELINES FOR RECYCLING OF PLASTICS\",\n",
    "    \"Union REGULATION (EU) 2020/852 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 18 June 2020\",\n",
    "    \"DIRECTIVE 2008/98/EC OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 19 November 2008 on waste and repealing certain Directives (Text with EEA relevance)\",\n",
    "    \"DIRECTIVES DIRECTIVE (EU) 2022/2464 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\",\n",
    "    \"REGULATION (EU) 2023/1542 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 12 July 2023 concerning batteries and waste batteries, amending Directive 2008/98/EC and Regulation (EU) 2019/1020 and repealing Directive 2006/66/EC (Text with EEA relevance)\",\n",
    "    \"EXPLANATORY MEMORANDUM COMMISSION DELEGATED REGULATION (EU) of 6.7.2021 supplementing Regulation (EU) 2020/852 of the European Parliament and of the Council\"\n",
    "]\n",
    "titles = list(set(titles) - set(excluded))\n",
    "print(len(titles))\n",
    "sorted(titles)"
   ],
   "id": "93fc0a64c3f3e145",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Act on Corporate Due Diligence Obligations for the Prevention of Human Rights Violations in Supply Chains (Lieferkettensorgfaltspflichtengesetz – LkSG)',\n",
       " 'OLD_acbj.com',\n",
       " 'OLD_adweek.com',\n",
       " 'OLD_amazon.com',\n",
       " 'OLD_archives.gov',\n",
       " 'OLD_buffalowildwings.com',\n",
       " 'OLD_cariboucoffee.com',\n",
       " 'OLD_cbsinteractive.com',\n",
       " 'OLD_communitycoffee.com',\n",
       " 'OLD_dailyillini.com',\n",
       " 'OLD_dogbreedinfo.com',\n",
       " 'OLD_eatchicken.com',\n",
       " 'OLD_education.jlab.org',\n",
       " 'OLD_fool.com',\n",
       " 'OLD_gawker.com',\n",
       " 'OLD_gwdocs.com',\n",
       " 'OLD_highgearmedia.com',\n",
       " 'OLD_honda.com',\n",
       " 'OLD_internetbrands.com',\n",
       " 'OLD_kaleidahealth.org',\n",
       " 'OLD_kraftrecipes.com',\n",
       " 'OLD_mlb.mlb.com',\n",
       " 'OLD_mohegansun.com',\n",
       " 'OLD_naturalnews.com',\n",
       " 'OLD_nbcuniversal.com',\n",
       " 'OLD_neworleansonline.com',\n",
       " 'OLD_opensecrets.org',\n",
       " 'OLD_post-gazette.com',\n",
       " 'OLD_reference.com',\n",
       " 'OLD_rockstargames.com',\n",
       " 'OLD_sci-news.com',\n",
       " 'OLD_sciencemag.org',\n",
       " 'OLD_si.edu',\n",
       " 'OLD_style.com',\n",
       " 'OLD_ted.com',\n",
       " 'OLD_theatlantic.com',\n",
       " 'OLD_ticketmaster.com',\n",
       " 'OLD_walmart.com',\n",
       " 'OLD_washingtonian.com',\n",
       " 'OLD_yahoo.com',\n",
       " 'OLD_zacks.com',\n",
       " 'adweek.com',\n",
       " 'buffalowildwings.com',\n",
       " 'cariboucoffee.com',\n",
       " 'cbsinteractive.com',\n",
       " 'dogbreedinfo.com',\n",
       " 'eatchicken.com',\n",
       " 'fool.com',\n",
       " 'internetbrands.com',\n",
       " 'kaleidahealth.org',\n",
       " 'mlb.mlb.com',\n",
       " 'naturalnews.com',\n",
       " 'post-gazette.com',\n",
       " 'sci-news.com',\n",
       " 'style.com',\n",
       " 'ted.com',\n",
       " 'theatlantic.com',\n",
       " 'ticketmaster.com',\n",
       " 'walmart.com',\n",
       " 'washingtonian.com',\n",
       " 'yahoo.com']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:01.676027Z",
     "start_time": "2024-07-11T09:10:01.627567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modules.embedding.service import get_all\n",
    "from modules.document.service import get_by_title\n",
    "title = \"Act on Corporate Due Diligence Obligations for the Prevention of Human Rights Violations in Supply Chains (Lieferkettensorgfaltspflichtengesetz – LkSG)\"\n",
    "document = get_by_title(title)\n",
    "embeds = get_all(title=title)\n",
    "len(embeds)"
   ],
   "id": "c6da6d40ba734e1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:52:19.132319Z",
     "start_time": "2024-07-11T09:52:19.128436Z"
    }
   },
   "cell_type": "code",
   "source": "len(embeds[2].text)",
   "id": "7f32be09a1201878",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:09.127541Z",
     "start_time": "2024-07-11T09:10:09.124044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc_for_qas = \"\\n\\n\".join([embed.text for embed in embeds[0:3]])\n",
    "doc_for_qas"
   ],
   "id": "9319acd8aa95ada7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\n\\nAct on Corporate Due Diligence Obligations in Supply Chains Of July 16 2021 The Bundestag has passed the following Act:\\n\\nArticle 1\\n\\nAct on Corporate Due Diligence Obligations for the Prevention of Human Rights Violations in Supply Chains (Lieferkettensorgfaltspflichtengesetz – LkSG)\\n\\nDivision 1 General provisions\\n\\nSection 1 Scope of application\\n\\n(1) This Act applies to enterprises regardless of their legal form that\\n\\n1. have their central administration, their principal place of business, their administrative headquarters or their statutory seat in Germany and\\n\\n2. that normally have at least 3,000 employees in Germany; employees posted abroad are included.\\n\\nNotwithstanding sentence 1 no. 1, this Act also applies to enterprises regardless of their legal form that\\n\\n1. have a domestic branch office pursuant to section 13d of the Commercial Code (Handelsgesetzbuch – HGB) and\\n\\n2. that normally have at least 3,000 employees in Germany.\\n\\nFrom 1 January 2024 the thresholds stipulated in sentence 1 no. 2 and sentence 2 no. 2 amount to 1,000 employees, respectively.\\n\\n(2) Temporary agency workers must be included in the calculation of the number of employees (paragraph (1) sentence 1 no. 2 and sentence 2 no. 2) of the user enterprise if the duration of the assignment exceeds six months.\\n\\n(3) Within affiliated enterprises (section 15 of the Stock Corporation Act [Aktiengesetz – AktG]), the employees of all enterprises belonging to the group who are employed in Germany must be taken into account when calculating the number of employees (paragraph (1) sentence 1 no. 2) of the parent company; employees posted abroad are included.\\n\\n2\\n\\nSection 2 Definitions\\n\\n(1) Protected legal positions within the meaning of this Act are those arising from the conventions on the protection of human rights listed in nos. 1 to 11 of the Annex.\\n\\n(2) A human rights risk within the meaning of this Act is a condition in which, on the basis of factual circumstances, there is a sufficient probability that a violation of one of the following prohibitions is imminent:\\n\\n1. the prohibition of the employment of a child under the age at which compulsory schooling ends according to the law of the place of employment, provided that the age of employment is not less than 15 years, except where the law of the place of employment so provides in accordance with Article 2 (4) and Articles 4 to 8 of Convention No. 138 of the International Labour Organization of 26 June 1973 concerning Minimum Age for Admission to Employment (Federal Law Gazette 1976 II pp. 201, 202);\\n\\n2. the prohibition of the worst forms of child labour for children under 18 years of age; in accordance with Article 3 of Convention No. 182 of the International Labour Organization of 17 June 1999 concerning the Prohibition and Immediate Action for the Elimination of the Worst Forms of Child Labour (Federal Law Gazette 2001 II pp. 1290, 1291) this includes:\\n\\na) all forms of slavery or practices similar to slavery, such as the sale and trafficking of children, debt bondage and serfdom, as well as forced or compulsory labour, including the forced or compulsory recruitment of children for use in armed conflicts,\\n\\nb) the use, procuring or offering of a child for prostitution, for the production of pornography or for pornographic performances,\\n\\nc) the use, procuring or offering of a child for illicit activities, in particular for the production of or trafficking in drugs,\\n\\nd) work which, by its nature or the circumstances in which it is carried out, is likely to harm the health, safety or morals of children;\\n\\n3. the prohibition of the employment of persons in forced labour; this includes any work or service that is required of a person under threat of punishment and for which he or she has not made himself or herself available voluntarily, for example as a result of debt bondage or trafficking in human beings; excluded from forced labour are any work or services that comply with Article 2 (2) of Convention No. 29 of the International Labour Organization of 28 June 1930 concerning Forced or Compulsory Labour (Federal Law Gazette 1956 II p. 640, 641) or with Article 8 (3) (b) and (c) of the International Covenant of 19 December 1966 on Civil and Political Rights (Federal Law Gazette 1973 II pp. 1533, 1534);'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:14.224536Z",
     "start_time": "2024-07-11T09:10:14.198610Z"
    }
   },
   "cell_type": "code",
   "source": "token_stats.get_stats(doc_for_qas)",
   "id": "9c39709bbe0caaa7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_tokens_per_page': 1.0,\n",
       " 'total_document_tokens': 4293,\n",
       " 'max_tokens_per_page': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:27.610997Z",
     "start_time": "2024-07-11T09:10:27.607996Z"
    }
   },
   "cell_type": "code",
   "source": "prompts = qa_gen_chain.prep_prompts([{\"doc\": doc_for_qas, \"num_questions\": num_questions, \"format_instructions\": format_instructions}])",
   "id": "873a864907515012",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:28.324538Z",
     "start_time": "2024-07-11T09:10:28.320834Z"
    }
   },
   "cell_type": "code",
   "source": "prompts[0]",
   "id": "d8d41c327fe27d55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringPromptValue(text='You are a legal expert and a teacher coming up with questions to ask on a quiz about ESG regulations. \\nGiven the following legal document, please generate 2 questions and corresponding answers based on that document. \\nGenerate only short questions without compound sentences. Generate a variety of questions that cover different aspects about the document. \\nImportant: In the questions use a variety of question words such as: why, how, where, when, who, which. \\n Here are some topic ideas for the questions: \\n    1. requirements for compliance\\n    2. penalties for non-compliance\\n    3. to whom the regulations apply\\n    4. criteria for applicability of this regulation\\n\\nOnly if the document does not contain information about those topics, include your own. \\nMake sure they are complete questions, and that they can be answered by extracting excerpts from the document. \\nEach answer must be composed of an exactly copied excerpt from the document.\\n\\nThese questions and answers should be detailed and be based explicitly on information in the document. \\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"qas\": {\"title\": \"Qas\", \"description\": \"List of questions and answers\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/QA\"}}}, \"required\": [\"qas\"], \"definitions\": {\"Answer\": {\"title\": \"Answer\", \"type\": \"object\", \"properties\": {\"text\": {\"title\": \"Text\", \"description\": \"Answer text\", \"type\": \"string\"}}, \"required\": [\"text\"]}, \"QA\": {\"title\": \"QA\", \"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"description\": \"Question text\", \"type\": \"string\"}, \"answers\": {\"title\": \"Answers\", \"description\": \"List of answers\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Answer\"}}}, \"required\": [\"question\", \"answers\"]}}}\\n``` \\nBegin! \\n\\n<Begin Document>\\n1\\n\\nAct on Corporate Due Diligence Obligations in Supply Chains Of July 16 2021 The Bundestag has passed the following Act:\\n\\nArticle 1\\n\\nAct on Corporate Due Diligence Obligations for the Prevention of Human Rights Violations in Supply Chains (Lieferkettensorgfaltspflichtengesetz – LkSG)\\n\\nDivision 1 General provisions\\n\\nSection 1 Scope of application\\n\\n(1) This Act applies to enterprises regardless of their legal form that\\n\\n1. have their central administration, their principal place of business, their administrative headquarters or their statutory seat in Germany and\\n\\n2. that normally have at least 3,000 employees in Germany; employees posted abroad are included.\\n\\nNotwithstanding sentence 1 no. 1, this Act also applies to enterprises regardless of their legal form that\\n\\n1. have a domestic branch office pursuant to section 13d of the Commercial Code (Handelsgesetzbuch – HGB) and\\n\\n2. that normally have at least 3,000 employees in Germany.\\n\\nFrom 1 January 2024 the thresholds stipulated in sentence 1 no. 2 and sentence 2 no. 2 amount to 1,000 employees, respectively.\\n\\n(2) Temporary agency workers must be included in the calculation of the number of employees (paragraph (1) sentence 1 no. 2 and sentence 2 no. 2) of the user enterprise if the duration of the assignment exceeds six months.\\n\\n(3) Within affiliated enterprises (section 15 of the Stock Corporation Act [Aktiengesetz – AktG]), the employees of all enterprises belonging to the group who are employed in Germany must be taken into account when calculating the number of employees (paragraph (1) sentence 1 no. 2) of the parent company; employees posted abroad are included.\\n\\n2\\n\\nSection 2 Definitions\\n\\n(1) Protected legal positions within the meaning of this Act are those arising from the conventions on the protection of human rights listed in nos. 1 to 11 of the Annex.\\n\\n(2) A human rights risk within the meaning of this Act is a condition in which, on the basis of factual circumstances, there is a sufficient probability that a violation of one of the following prohibitions is imminent:\\n\\n1. the prohibition of the employment of a child under the age at which compulsory schooling ends according to the law of the place of employment, provided that the age of employment is not less than 15 years, except where the law of the place of employment so provides in accordance with Article 2 (4) and Articles 4 to 8 of Convention No. 138 of the International Labour Organization of 26 June 1973 concerning Minimum Age for Admission to Employment (Federal Law Gazette 1976 II pp. 201, 202);\\n\\n2. the prohibition of the worst forms of child labour for children under 18 years of age; in accordance with Article 3 of Convention No. 182 of the International Labour Organization of 17 June 1999 concerning the Prohibition and Immediate Action for the Elimination of the Worst Forms of Child Labour (Federal Law Gazette 2001 II pp. 1290, 1291) this includes:\\n\\na) all forms of slavery or practices similar to slavery, such as the sale and trafficking of children, debt bondage and serfdom, as well as forced or compulsory labour, including the forced or compulsory recruitment of children for use in armed conflicts,\\n\\nb) the use, procuring or offering of a child for prostitution, for the production of pornography or for pornographic performances,\\n\\nc) the use, procuring or offering of a child for illicit activities, in particular for the production of or trafficking in drugs,\\n\\nd) work which, by its nature or the circumstances in which it is carried out, is likely to harm the health, safety or morals of children;\\n\\n3. the prohibition of the employment of persons in forced labour; this includes any work or service that is required of a person under threat of punishment and for which he or she has not made himself or herself available voluntarily, for example as a result of debt bondage or trafficking in human beings; excluded from forced labour are any work or services that comply with Article 2 (2) of Convention No. 29 of the International Labour Organization of 28 June 1930 concerning Forced or Compulsory Labour (Federal Law Gazette 1956 II p. 640, 641) or with Article 8 (3) (b) and (c) of the International Covenant of 19 December 1966 on Civil and Political Rights (Federal Law Gazette 1973 II pp. 1533, 1534);\\n<End Document>')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:32.878389Z",
     "start_time": "2024-07-11T09:10:32.842806Z"
    }
   },
   "cell_type": "code",
   "source": "token_stats.get_stats(prompts[0][0].text)",
   "id": "9e89170ed83bcdf9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_tokens_per_page': 1.0,\n",
       " 'total_document_tokens': 6497,\n",
       " 'max_tokens_per_page': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:10:35.593260Z",
     "start_time": "2024-07-11T09:10:35.590393Z"
    }
   },
   "cell_type": "code",
   "source": "print([f'{title}\\n' for title in titles])",
   "id": "bc5a9ac8ffc9c6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OLD_kraftrecipes.com\\n', 'OLD_opensecrets.org\\n', 'OLD_yahoo.com\\n', 'OLD_buffalowildwings.com\\n', 'walmart.com\\n', 'yahoo.com\\n', 'OLD_washingtonian.com\\n', 'OLD_reference.com\\n', 'kaleidahealth.org\\n', 'OLD_kaleidahealth.org\\n', 'cbsinteractive.com\\n', 'OLD_cariboucoffee.com\\n', 'OLD_sciencemag.org\\n', 'ticketmaster.com\\n', 'theatlantic.com\\n', 'adweek.com\\n', 'OLD_post-gazette.com\\n', 'sci-news.com\\n', 'cariboucoffee.com\\n', 'OLD_walmart.com\\n', 'OLD_dogbreedinfo.com\\n', 'OLD_internetbrands.com\\n', 'OLD_sci-news.com\\n', 'ted.com\\n', 'OLD_naturalnews.com\\n', 'OLD_theatlantic.com\\n', 'OLD_ted.com\\n', 'Act on Corporate Due Diligence Obligations for the Prevention of Human Rights Violations in Supply Chains (Lieferkettensorgfaltspflichtengesetz – LkSG)\\n', 'post-gazette.com\\n', 'OLD_neworleansonline.com\\n', 'OLD_adweek.com\\n', 'OLD_education.jlab.org\\n', 'OLD_gwdocs.com\\n', 'OLD_acbj.com\\n', 'OLD_si.edu\\n', 'OLD_eatchicken.com\\n', 'OLD_mlb.mlb.com\\n', 'OLD_honda.com\\n', 'OLD_communitycoffee.com\\n', 'style.com\\n', 'OLD_rockstargames.com\\n', 'OLD_cbsinteractive.com\\n', 'OLD_archives.gov\\n', 'OLD_ticketmaster.com\\n', 'OLD_gawker.com\\n', 'OLD_nbcuniversal.com\\n', 'eatchicken.com\\n', 'OLD_dailyillini.com\\n', 'OLD_zacks.com\\n', 'buffalowildwings.com\\n', 'OLD_mohegansun.com\\n', 'dogbreedinfo.com\\n', 'fool.com\\n', 'OLD_fool.com\\n', 'naturalnews.com\\n', 'internetbrands.com\\n', 'OLD_highgearmedia.com\\n', 'OLD_amazon.com\\n', 'washingtonian.com\\n', 'mlb.mlb.com\\n', 'OLD_style.com\\n']\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:03.197112Z",
     "start_time": "2024-07-11T09:11:03.194517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data generation loop\n",
    "from uuid import uuid4\n",
    "import json\n",
    "import os\n",
    "#\n",
    "file_path = \"../data/generated/generated_qas_results.json\"\n",
    "#\n",
    "#data = {\"data\": []}\n",
    "#if os.path.exists(file_path):\n",
    "#    with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "#        data = json.load(f)\n",
    "#\n",
    "#data_item = {\n",
    "#    \"source\": document.source,\n",
    "#    \"title\": document.title,\n",
    "#    \"paragraphs\": []\n",
    "#}\n",
    "#data[\"data\"].append(data_item)\n",
    "#step = 3\n",
    "##for i in range(0, len(embeds), step):\n",
    "#for i in range(0, len(embeds), step):\n",
    "#    current_embeds = embeds[i:i+step]\n",
    "#    doc_for_qas = str(\"\\n\\n\".join([embed.text for embed in current_embeds]))\n",
    "#    page_numbers = list(sorted(set([embed.page_number for embed in current_embeds])))\n",
    "#    embed_ids = list(str(embed.id) for embed in current_embeds)\n",
    "#    generated_qas = qa_gen_chain.apply(\n",
    "#        [{\"doc\": doc_for_qas, \"num_questions\": num_questions, \"format_instructions\": format_instructions}]\n",
    "#    )\n",
    "#    paragraph = {\n",
    "#        \"qas\": generated_qas[0][\"response\"][\"qas\"],\n",
    "#        \"context\": doc_for_qas,\n",
    "#        \"embedding_ids\": embed_ids,\n",
    "#        \"page_numbers\": page_numbers\n",
    "#    }\n",
    "#    \n",
    "#    for qa in paragraph[\"qas\"]:\n",
    "#        qa[\"id\"] = str(uuid4())\n",
    "#    \n",
    "#    data_item[\"paragraphs\"].append(paragraph)\n",
    "#    print(f\"saving data to {file_path}, pages: {page_numbers}\")\n",
    "#    with open(file_path, \"w\", encoding=\"utf8\") as f:\n",
    "#        json.dump(data, f, indent=4, ensure_ascii=False)"
   ],
   "id": "56d5c5c2b09be495",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:03.742555Z",
     "start_time": "2024-07-11T09:11:03.740404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#generated_qas = qa_gen_chain.apply(\n",
    "#    [{\"doc\": doc_for_qas, \"num_questions\": num_questions, \"format_instructions\": format_instructions}]\n",
    "#)#[qa_gen_chain.output_key]"
   ],
   "id": "52d625b7ea90fb41",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:04.716210Z",
     "start_time": "2024-07-11T09:11:04.713160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#generated_qas[0][qa_gen_chain.output_key]\n",
    "#generated_qas\n",
    "\n",
    "#for item in generated_qas[0][\"response\"][\"qas\"]:\n",
    "#    item[\"context\"] = \"inserted context\"\n",
    "#    \n",
    "#generated_qas    "
   ],
   "id": "3b1707d14b568ce",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:06.354889Z",
     "start_time": "2024-07-11T09:11:06.351384Z"
    }
   },
   "cell_type": "code",
   "source": "#token_stats.get_stats(str(generated_qas))",
   "id": "748b21f23f3f9a04",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:23.532742Z",
     "start_time": "2024-07-11T09:11:23.462445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate number of questions and answers\n",
    "\n",
    "from modules.document.utils.policyqa_parser import *\n",
    "\n",
    "file_path = \"../data/generated/generated_qas_clean.json\"\n",
    "\n",
    "\n",
    "num_questions = count_questions(file_path)\n",
    "num_answers = count_answers(file_path)\n",
    "avg_answer_size = calculate_avg_answer_size(file_path)\n",
    "min_answer_size, max_answer_size = calculate_min_max_answer_size(file_path)\n",
    "min_context_size, max_context_size = calculate_min_max_context_size(file_path)\n",
    "\n",
    "print(f\"Number of questions: {num_questions}\")\n",
    "print(f\"Number of answers: {num_answers}\")\n",
    "print(f\"Average answer size: {avg_answer_size}\")\n",
    "print(f\"Min answer size: {min_answer_size}\")\n",
    "print(f\"Max answer size: {max_answer_size}\")\n",
    "print(f\"Min context size: {min_context_size}\")\n",
    "print(f\"Max context size: {max_context_size}\")"
   ],
   "id": "ae3bf2b0837d5730",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 536\n",
      "Number of answers: 536\n",
      "Average answer size: 328.0186567164179\n",
      "Min answer size: 4\n",
      "Max answer size: 1252\n",
      "Min context size: 36\n",
      "Max context size: 5791\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check whether all answers are in the context",
   "id": "4d98cb0e6685a070"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:45.636042Z",
     "start_time": "2024-07-11T09:11:45.602056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_numbers_in_parentheses(s):\n",
    "    return re.sub(r'\\(\\d{1,2}\\)', '', s)\n",
    "\n",
    "def replace_multiple_spaces(s):\n",
    "    return re.sub(' +', ' ', s)\n",
    "\n",
    "file_path = \"../data/generated/generated_qas_clean.json\"\n",
    "\n",
    "answers_not_in_context = []\n",
    "answers_in_context = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "    count = 0\n",
    "    data = json.load(f)\n",
    "    for item in data[\"data\"]:\n",
    "        title = item[\"title\"]\n",
    "        source = item[\"source\"]\n",
    "        for paragraph in item[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_id = qa[\"id\"]\n",
    "                question_text = str(qa[\"question\"])\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    answer_text = str(answer[\"text\"])\n",
    "                    context = str(paragraph[\"context\"])\n",
    "                    #answer_text = answer_text.replace(\"\\n\", \" \")\n",
    "                    #context = context.replace(\"\\n\", \" \")\n",
    "                    #answer_text = replace_multiple_spaces(answer_text)\n",
    "                    #context = replace_multiple_spaces(context)\n",
    "                    #answer_text = remove_numbers_in_parentheses(answer_text)\n",
    "                    #context = remove_numbers_in_parentheses(context)\n",
    "                    answer_text = answer_text.lower()\n",
    "                    context = context.lower()\n",
    "                    answer_text = \" \".join(answer_text.split())\n",
    "                    context = \" \".join(context.split())\n",
    "                    if answer_text.endswith(\".\"):\n",
    "                        answer_text = answer_text[:-1]\n",
    "                    if answer_text not in context:\n",
    "                        print(f\"##############\")\n",
    "                        print(f\"Answer not in context for question: {qa['id']}\")\n",
    "                        print(f\"##############\")\n",
    "                        print(answer_text)\n",
    "                        print(f\"##############\")\n",
    "                        print(context)\n",
    "                        count += 1\n",
    "                        answers_not_in_context.append({\n",
    "                            \"id\": question_id,\n",
    "                            \"question\": question_text,\n",
    "                            \"answer\": answer_text,\n",
    "                            \"context\": context,\n",
    "                            \"title\": title,\n",
    "                            \"source\": source\n",
    "                        })\n",
    "                    else:\n",
    "                        answers_in_context.append({\n",
    "                            \"id\": question_id,\n",
    "                            \"question\": question_text,\n",
    "                            \"answer\": answer_text,\n",
    "                            \"context\": context,\n",
    "                            \"title\": title,\n",
    "                            \"source\": source\n",
    "                        })\n",
    "                        \n",
    "    print(f\"Total number of answers not in context: {count}\")                        "
   ],
   "id": "61e103f50097fa81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of answers not in context: 0\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:47.919086Z",
     "start_time": "2024-07-11T09:11:47.911084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "answer_not_in_context_df = pd.DataFrame(answers_not_in_context)\n",
    "answer_in_context_df = pd.DataFrame(answers_in_context)\n",
    "\n",
    "len(answer_not_in_context_df)"
   ],
   "id": "4f0847c650996faf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:49.476650Z",
     "start_time": "2024-07-11T09:11:48.988065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "n = 0\n",
    "for i in range(n, n+1):\n",
    "    answer = \"\"\n",
    "    context = answer_not_in_context_df.iloc[i][\"context\"]\n",
    "    source = answer_not_in_context_df.iloc[i][\"source\"]\n",
    "    found_index = -1\n",
    "    q_id = answer_not_in_context_df.iloc[i][\"id\"]\n",
    "    for word in answer_not_in_context_df.iloc[i][\"answer\"].split():\n",
    "        answer += f\"{word} \"\n",
    "        try:\n",
    "            found_index = context.index(answer)\n",
    "        except ValueError:\n",
    "            answer_len = len(answer)\n",
    "            print(f\"source: {source}\")\n",
    "            print(f\"question: {answer_not_in_context_df.iloc[i]['question']}\")\n",
    "            print(f\"for question id: {q_id}\")\n",
    "            print(f\"GT answer: {answer_not_in_context_df.iloc[i]['answer']}\")\n",
    "            print(f\"answer: {answer[:-1]}\")\n",
    "            print(f\"context: {answer_not_in_context_df.iloc[i]['context'][found_index:found_index+answer_len]}\")\n",
    "            break"
   ],
   "id": "2a3d904d5dbdee26",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n, n\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      3\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 4\u001B[0m     context \u001B[38;5;241m=\u001B[39m \u001B[43manswer_not_in_context_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      5\u001B[0m     source \u001B[38;5;241m=\u001B[39m answer_not_in_context_df\u001B[38;5;241m.\u001B[39miloc[i][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      6\u001B[0m     found_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   1189\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[0;32m   1190\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001B[1;32m-> 1191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\pandas\\core\\indexing.py:1752\u001B[0m, in \u001B[0;36m_iLocIndexer._getitem_axis\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot index by location index with a non-integer key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1751\u001B[0m \u001B[38;5;66;03m# validate the location\u001B[39;00m\n\u001B[1;32m-> 1752\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_integer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1754\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_ixs(key, axis\u001B[38;5;241m=\u001B[39maxis)\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\pandas\\core\\indexing.py:1685\u001B[0m, in \u001B[0;36m_iLocIndexer._validate_integer\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1683\u001B[0m len_axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_axis(axis))\n\u001B[0;32m   1684\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m len_axis \u001B[38;5;129;01mor\u001B[39;00m key \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m-\u001B[39mlen_axis:\n\u001B[1;32m-> 1685\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msingle positional indexer is out-of-bounds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mIndexError\u001B[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:53.876398Z",
     "start_time": "2024-07-11T09:11:53.503269Z"
    }
   },
   "cell_type": "code",
   "source": "answer_not_in_context_df.loc[answer_not_in_context_df[\"id\"] == q_id]",
   "id": "c7c7caff8ea853c5",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m answer_not_in_context_df\u001B[38;5;241m.\u001B[39mloc[\u001B[43manswer_not_in_context_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m==\u001B[39m q_id]\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    415\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Hashable):\n\u001B[1;32m--> 417\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n\u001B[0;32m    418\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'id'"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:55.048804Z",
     "start_time": "2024-07-11T09:11:55.038256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "                        \"47c82065-a2f5-4700-b40b-1db16da9c1b7\",\n",
    "                        \"6c843a56-2236-4aa8-b002-d16d22613364\",\n",
    "                        \"bdcd1153-904a-45ed-ac8a-e58775448158\"\n",
    "\"\"\"\n",
    "\n",
    "from modules.embedding.service import get as get_embedding_by_id\n",
    "\n",
    "embedding = get_embedding_by_id(\"3a2051de-fb9e-40fc-820d-689bc7c989b6\")\n",
    "print(embedding.text)"
   ],
   "id": "71e8b6a9f78d93dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(c) facilitating the preparation for re-use, preparation for repurposing, repurposing or remanufacturing of the battery.\n",
      "\n",
      "3. The battery management system shall include a software reset function, in case economic operators carrying out preparation for re-use, preparation for repurposing, repurposing or remanufacturing need to upload different battery management system software. If the software reset function is used, the original battery manufacturer shall not be held liable for any breach of the safety or functionality of the battery that could be attributed to battery management system software uploaded after that battery was placed on the market.\n",
      "\n",
      "CHAPTER IV\n",
      "\n",
      "Conformity of batteries\n",
      "\n",
      "Article 15\n",
      "\n",
      "Presumption of conformity of batteries\n",
      "\n",
      "Official Journal of the European Union\n",
      "\n",
      "2. Harmonised standards shall aim to simulate real-life usage as far as possible while maintaining standard tests.\n",
      "\n",
      "Official Journal of the European Union\n",
      "\n",
      "Article 16\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Remove questions with answers not in context",
   "id": "9ca16a600a912cfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:11:59.246001Z",
     "start_time": "2024-07-11T09:11:59.221639Z"
    }
   },
   "cell_type": "code",
   "source": "len(answer_not_in_context_df[\"id\"].tolist())",
   "id": "844a61a0865dc425",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mlen\u001B[39m(\u001B[43manswer_not_in_context_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mtolist())\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    415\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Hashable):\n\u001B[1;32m--> 417\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n\u001B[0;32m    418\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'id'"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:00.138334Z",
     "start_time": "2024-07-11T09:12:00.115931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for item in data[\"data\"]:\n",
    "    for paragraph in item[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if qa[\"id\"] in answer_not_in_context_df[\"id\"].tolist():\n",
    "                paragraph[\"qas\"].remove(qa)\n",
    "\n",
    "#file = \"../data/generated/generated_qas_clean.json\"\n",
    "#with open(file, \"w\", encoding=\"utf8\") as f:\n",
    "#    json.dump(data, f, indent=4, ensure_ascii=False)"
   ],
   "id": "dcc7afb0d31381d2",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m paragraph \u001B[38;5;129;01min\u001B[39;00m item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparagraphs\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m qa \u001B[38;5;129;01min\u001B[39;00m paragraph[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mqas\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m----> 4\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m qa[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m \u001B[43manswer_not_in_context_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mtolist():\n\u001B[0;32m      5\u001B[0m                 paragraph[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mqas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mremove(qa)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m#file = \"../data/generated/generated_qas_clean.json\"\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m#with open(file, \"w\", encoding=\"utf8\") as f:\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m#    json.dump(data, f, indent=4, ensure_ascii=False)\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai4esg-MyrZEbvC-py3.11\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    415\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Hashable):\n\u001B[1;32m--> 417\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n\u001B[0;32m    418\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'id'"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Remove questions with answers that span multiple embeddings",
   "id": "49241ea8d6fdeed1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:02.641664Z",
     "start_time": "2024-07-11T09:12:02.637702Z"
    }
   },
   "cell_type": "code",
   "source": "qs_span_multiple_embeds = ['7ae57dda-b9b1-4e88-be53-a3aaec5a26db', '17f2546f-7a64-43d9-a47f-abc952f91f49', '226111ad-7262-401f-869a-66fc331232bc', '39c1ac2b-b524-4b1c-8861-64f58a3b3b67', '5a43cfb3-2af7-4035-bb4a-4992ca7090e6', '512f0b9d-488f-44bc-8e46-d6af4a3b7069', '60fdca96-3b36-4cd2-8b7a-608734e2ea46', '3c770ce8-6c72-4021-83e3-c98ae17ec745', 'e042d03e-3d23-4183-96e2-96d9cd13a131', 'a7c29aa9-21c6-40d7-a907-e60c83f7f28e', '164d4bdf-a4a4-4922-ab83-50e1ebec34e7', '121c1e65-5c26-4d7d-bc58-6ab3abf1b9dd', '461f01a6-a6bf-4946-bb12-283cfa269541', 'b4b5945a-caf4-4a90-9577-e7255a0344a9', '22f765a6-944d-496b-a796-66f7d31f4f5e', '483bec64-f7b7-4e8e-9059-a33cf0d3c3ea', '001d7c6e-e5a7-4583-9c96-fe3e8cc366ec', 'e3497c6b-3d73-4d27-ad2f-4a05c3cd6da4', 'dcaf8486-4784-46a6-9be2-4ab41553dba5', '7061fc8e-deea-4ad1-bc9b-222c9717ed35', '3a84bc67-dd00-47f9-be65-6413a698f173', '015b9098-1455-40b1-81b8-fb0297d346f1', '9c42f0ef-8e47-4256-87bb-8368420e3a44', '29685aa7-d02e-494f-a66d-4f9ea5e351cd', '4795f35e-0db5-4ddc-828f-a93f78dc8c65', '43abe044-236c-414a-aee3-6e211639bd6d', '8a40e6aa-deaf-4974-84c8-6c1678a54600', '48b23883-f9bc-4f1a-9425-9a219b0ed93d', 'caec996c-8e2d-4cbc-99cf-7647193f0e4d', 'ce0d00a5-1978-4235-a494-b4c3866e2119', 'ef18c7b7-59bd-46e3-802b-a7efb22b79fe', 'ebbbafce-a6f8-468a-9906-63305a1adfea', 'b9740a36-b7b0-42e6-9d17-4f3e4c5bbebe']",
   "id": "d6f5bacd3edbfd1b",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:27.752569Z",
     "start_time": "2024-07-11T09:12:27.749126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for item in data[\"data\"]:\n",
    "    for paragraph in item[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if qa[\"id\"] in qs_span_multiple_embeds:\n",
    "                paragraph[\"qas\"].remove(qa)\n",
    "\n",
    "#file = \"../data/generated/generated_qas_clean.json\"\n",
    "#with open(file, \"w\", encoding=\"utf8\") as f:\n",
    "#    json.dump(data, f, indent=4, ensure_ascii=False)"
   ],
   "id": "2f8f12c21efe3f97",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Remove questions with less than 3 embeddings",
   "id": "38ec3545f7217069"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:30.667837Z",
     "start_time": "2024-07-11T09:12:30.664408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_less_than_3_embeds = []\n",
    "\n",
    "for item in data[\"data\"]:\n",
    "    for paragraph in item[\"paragraphs\"]:\n",
    "        if len(paragraph[\"embedding_ids\"]) < 3:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qa_less_than_3_embeds.append(qa[\"id\"])"
   ],
   "id": "cca6e14ad9a7c565",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:32.148913Z",
     "start_time": "2024-07-11T09:12:32.146174Z"
    }
   },
   "cell_type": "code",
   "source": "print([id for id in qa_less_than_3_embeds])",
   "id": "db8ec38d20fdcbdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:42.149933Z",
     "start_time": "2024-07-11T09:12:42.146448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for item in data[\"data\"]:\n",
    "    for paragraph in item[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if qa[\"id\"] in qa_less_than_3_embeds:\n",
    "                paragraph[\"qas\"].remove(qa)\n",
    "\n",
    "#file = \"../data/generated/generated_qas_clean.json\"\n",
    "#with open(file, \"w\", encoding=\"utf8\") as f:\n",
    "#    json.dump(data, f, indent=4, ensure_ascii=False)"
   ],
   "id": "23bd2d0824c09431",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split data into train and test",
   "id": "c39705979e11dce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:48.133964Z",
     "start_time": "2024-07-11T09:12:48.120370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modules.document.utils.split_utils import create_qa_train_test_split\n",
    "\n",
    "#create_qa_train_test_split(file_path, split=0.51)"
   ],
   "id": "84d39cfa79068a0e",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Verify no questions with less than 3 embeddings after split",
   "id": "f8aaeadb36cd870"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:53.858055Z",
     "start_time": "2024-07-11T09:12:53.821356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = json.load(open(\"../data/generated/train.json\", \"r\", encoding=\"utf-8\"))\n",
    "test_data = json.load(open(\"../data/generated/test.json\", \"r\", encoding=\"utf-8\"))"
   ],
   "id": "909c2225f8a7c379",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:54.845884Z",
     "start_time": "2024-07-11T09:12:54.842605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_qs_to_remove = []\n",
    "\n",
    "for item in train_data[\"data\"]:\n",
    "    for paragraph in item[\"paragraphs\"]:\n",
    "        if len(paragraph[\"embedding_ids\"]) < 3:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                train_qs_to_remove.append(qa[\"id\"])"
   ],
   "id": "c983ef085a9855a1",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:55.594696Z",
     "start_time": "2024-07-11T09:12:55.591836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_qs_to_remove = []\n",
    "\n",
    "for item in test_data[\"data\"]:\n",
    "    for paragraph in item[\"paragraphs\"]:\n",
    "        if len(paragraph[\"embedding_ids\"]) < 3:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                test_qs_to_remove.append(qa[\"id\"])"
   ],
   "id": "3a532ee6f7d04456",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:12:56.737544Z",
     "start_time": "2024-07-11T09:12:56.734582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print([id for id in train_qs_to_remove])\n",
    "print([id for id in test_qs_to_remove])"
   ],
   "id": "9a099c2a681b86c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train, Test stats",
   "id": "84e981c62bf435d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:13:02.600378Z",
     "start_time": "2024-07-11T09:13:02.567304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate number of questions and answers\n",
    "\n",
    "from modules.document.utils.policyqa_parser import *\n",
    "\n",
    "file_path = \"../data/generated/train.json\"\n",
    "\n",
    "\n",
    "num_questions = count_questions(file_path)\n",
    "num_answers = count_answers(file_path)\n",
    "avg_answer_size = calculate_avg_answer_size(file_path)\n",
    "min_answer_size, max_answer_size = calculate_min_max_answer_size(file_path)\n",
    "min_context_size, max_context_size = calculate_min_max_context_size(file_path)\n",
    "\n",
    "print(\"train dataset stats\")\n",
    "print(f\"Number of questions: {num_questions}\")\n",
    "print(f\"Number of answers: {num_answers}\")\n",
    "print(f\"Average answer size: {avg_answer_size}\")\n",
    "print(f\"Min answer size: {min_answer_size}\")\n",
    "print(f\"Max answer size: {max_answer_size}\")\n",
    "print(f\"Min context size: {min_context_size}\")\n",
    "print(f\"Max context size: {max_context_size}\")"
   ],
   "id": "a152b03ff2f3f315",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset stats\n",
      "Number of questions: 273\n",
      "Number of answers: 273\n",
      "Average answer size: 326.5274725274725\n",
      "Min answer size: 4\n",
      "Max answer size: 1252\n",
      "Min context size: 36\n",
      "Max context size: 5791\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:13:08.887345Z",
     "start_time": "2024-07-11T09:13:08.851378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate number of questions and answers\n",
    "\n",
    "from modules.document.utils.policyqa_parser import *\n",
    "\n",
    "file_path = \"../data/generated/test.json\"\n",
    "\n",
    "\n",
    "num_questions = count_questions(file_path)\n",
    "num_answers = count_answers(file_path)\n",
    "avg_answer_size = calculate_avg_answer_size(file_path)\n",
    "min_answer_size, max_answer_size = calculate_min_max_answer_size(file_path)\n",
    "min_context_size, max_context_size = calculate_min_max_context_size(file_path)\n",
    "\n",
    "print(\"test dataset stats\")\n",
    "print(f\"Number of questions: {num_questions}\")\n",
    "print(f\"Number of answers: {num_answers}\")\n",
    "print(f\"Average answer size: {avg_answer_size}\")\n",
    "print(f\"Min answer size: {min_answer_size}\")\n",
    "print(f\"Max answer size: {max_answer_size}\")\n",
    "print(f\"Min context size: {min_context_size}\")\n",
    "print(f\"Max context size: {max_context_size}\")"
   ],
   "id": "be96533e4aed02e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset stats\n",
      "Number of questions: 263\n",
      "Number of answers: 263\n",
      "Average answer size: 329.5665399239544\n",
      "Min answer size: 15\n",
      "Max answer size: 1091\n",
      "Min context size: 36\n",
      "Max context size: 5791\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check if we get the same result from GPT4 given the context",
   "id": "eb48980ce29ad08c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:13:42.765751Z",
     "start_time": "2024-07-11T09:13:42.755688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "len(answer_in_context_df)\n",
    "#get question and answers from training data\n",
    "\n",
    "file_path = \"../data/generated/train.json\"\n",
    "\n",
    "train_q_ids = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "    for item in data[\"data\"]:\n",
    "        title = item[\"title\"]\n",
    "        source = item[\"source\"]\n",
    "        for paragraph in item[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                train_q_ids.append(qa[\"id\"])\n",
    "\n",
    "print(\"number of questions in training data: \", len(train_q_ids))"
   ],
   "id": "1d50af44c87c1218",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of questions in training data:  273\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:13:44.005385Z",
     "start_time": "2024-07-11T09:13:44.000462Z"
    }
   },
   "cell_type": "code",
   "source": "train_answers_in_context_df = answer_in_context_df.loc[answer_in_context_df[\"id\"].isin(train_q_ids)]",
   "id": "d362f1b08cefb9cd",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:13:44.805745Z",
     "start_time": "2024-07-11T09:13:44.801798Z"
    }
   },
   "cell_type": "code",
   "source": "len(train_answers_in_context_df)",
   "id": "acdc2508ab16f822",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:13:49.449629Z",
     "start_time": "2024-07-11T09:13:49.446541Z"
    }
   },
   "cell_type": "code",
   "source": "train_q_ids_in_context = train_answers_in_context_df[\"id\"].tolist()",
   "id": "ac27498de7b14a0",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:13:50.178735Z",
     "start_time": "2024-07-11T09:13:50.175387Z"
    }
   },
   "cell_type": "code",
   "source": "len(train_q_ids_in_context)",
   "id": "b5aed26bef43771e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Upper bound evaluation",
   "id": "47a3a5a65a1875cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATASET = \"train\"\n",
    "DATASET_FILE = f\"../data/generated/{DATASET}.json\"\n",
    "RESULT_FILE = f\"../data/generated/results/upper_bound_{DATASET}_v2.json\"\n",
    "\n",
    "print(f'DATASET_FILE: {DATASET_FILE}')\n",
    "print(f'RESULT_FILE: {RESULT_FILE}')"
   ],
   "id": "a168370cb7cd991c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modules.llm.clients.openai.azure_openai_client import AzureOpenAILLMClient\n",
    "from modules.llm.clients.open_source.local_llm_client import LocalLLMClient\n",
    "from modules.llm.llm_infos import Model"
   ],
   "id": "d51c406df95c5489",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modules.answer.schemas import AnswerCreate\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from collections import deque\n",
    "from modules.embedding.service import get as get_embedding_by_id\n",
    "\n",
    "#rate_limit_per_minute = 9600 # values from azure\n",
    "#tokens_per_minute = 40000 # values from azure\n",
    "rate_limit_per_minute = float(\"inf\") # local values\n",
    "tokens_per_minute = float(\"inf\") # local values\n",
    "delay = 60.0 / rate_limit_per_minute\n",
    "\n",
    "def get_saved_qas(file_path: str):\n",
    "    saved_qas = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "            if \"data\" in json_data:\n",
    "                for qa in json_data[\"data\"]:\n",
    "                    saved_qas.append(qa[\"id\"])\n",
    "    return saved_qas\n",
    "\n",
    "def filter_embeddings(embeddings, answers):\n",
    "    relevant_embeddings = [embedding for embedding in embeddings if any(answer for answer in answers if answer['text'] in embedding.text)]\n",
    "    if len(relevant_embeddings) > 3:\n",
    "        relevant_embeddings = relevant_embeddings[:3]\n",
    "    return relevant_embeddings\n",
    "\n",
    "def filter_embeddings_context(embeddings, context):\n",
    "    relevant_embeddings = [embedding for embedding in embeddings if context == embedding.text]\n",
    "    return relevant_embeddings\n",
    "\n",
    "#def filter_embedding_with_answer(embeddings, answer):\n",
    "#    if answer.endswith(\".\"):\n",
    "#        answer = answer[:-1]\n",
    "#    answer = \" \".join(answer.split())\n",
    "#    filtered_embeddings = [embedding for embedding in embeddings if answer.lower()\n",
    "#                           in \" \".join(embedding.text.lower().replace(\"\\n\", \" \").split())]\n",
    "#    return filtered_embeddings\n",
    "\n",
    "def filter_embedding_with_answer(embeddings, answer):\n",
    "    if answer.endswith(\".\"):\n",
    "        answer = answer[:-1]\n",
    "    answer = \" \".join(answer.split())    \n",
    "    filtered_embeddings = [embedding for embedding in embeddings if answer.lower()\n",
    "                           in \" \".join(embedding.text.lower().replace(\"\\n\", \" \").split())]\n",
    "    return filtered_embeddings\n",
    "\n",
    "def map_relevant_embeddings(embeddings):\n",
    "    relevant_embeddings = []\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        relevant_embedding = {}\n",
    "        relevant_embedding[\"embedding_id\"] = str(embedding.id)\n",
    "        relevant_embedding[\"rank\"] = i+1\n",
    "        relevant_embedding[\"title\"] = embedding.document.title\n",
    "        relevant_embedding[\"offset\"] = embedding.offset\n",
    "        relevant_embedding[\"text\"] = embedding.text\n",
    "        relevant_embedding[\"score\"] = 0.0\n",
    "        relevant_embeddings.append(relevant_embedding)\n",
    "    return relevant_embeddings\n",
    "\n",
    "async def run_all_qas(file_path: str, model: Model=Model.AZURE_GPT4) -> None:\n",
    "    qs_multiple_embeds = [] #remove later\n",
    "    running_tokens = 0\n",
    "    total_tokens = 0\n",
    "    start_time = time.time()\n",
    "    results = {\"data\": []}\n",
    "\n",
    "    # check if there are saved results and load them\n",
    "    saved_qas = get_saved_qas(RESULT_FILE)\n",
    "    if len(saved_qas) > 0:\n",
    "        with open(RESULT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            results = json.load(f)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "        if \"data\" in json_data:\n",
    "            for regulation in tqdm(json_data[\"data\"]):\n",
    "                title = regulation[\"title\"]\n",
    "                logging.info(f\"processing regulation: {title}\")\n",
    "                for i, paragraph in tqdm(enumerate(regulation[\"paragraphs\"])):\n",
    "                    context = paragraph[\"context\"]\n",
    "                    for qa in paragraph[\"qas\"]:\n",
    "                        q_id = qa[\"id\"]\n",
    "                        if q_id in saved_qas:\n",
    "                            print(f\"skipping question {q_id} because it is already saved\")\n",
    "                            continue\n",
    "                        question = qa[\"question\"]\n",
    "                        answers = qa[\"answers\"]\n",
    "                        embed_ids = paragraph[\"embedding_ids\"]\n",
    "                        request = AnswerCreate(\n",
    "                            question=question,\n",
    "                            model=model,\n",
    "                            prompt=f\"\"\"You are a legal expert in ESG regulation, please give a concise answer to the question based on the given context. The answer must be an exact excerpt extracted from the given context that answers the question. Make sure to not add any additional or irrelevant information to the answer. Do not add quotes around the answer\"\"\"\n",
    "                        )\n",
    "\n",
    "                        if running_tokens > tokens_per_minute*0.8 and time.time() - start_time < 60.0:\n",
    "                            time.sleep((time.time() - start_time)+15)\n",
    "                            start_time = time.time()\n",
    "                            running_tokens = 0\n",
    "\n",
    "                        ## switch here to get embeddings from context or from document\n",
    "                        #filtered_embeddings = filter_embeddings(embeddings, answers)\n",
    "                        filtered_embeddings = [get_embedding_by_id(embed_id) for embed_id in embed_ids]\n",
    "                        filtered_embeddings = filter_embedding_with_answer(filtered_embeddings, answers[0][\"text\"])\n",
    "                        if len(filtered_embeddings) == 0:\n",
    "                            print(f\"no embeddings found for question {q_id}\")\n",
    "                        #filtered_embeddings = filter_embeddings_context(embeddings, context)\n",
    "                        ## only remote api model is GPT4\n",
    "                        if model == Model.AZURE_GPT4:\n",
    "                            llm_client = AzureOpenAILLMClient(model)\n",
    "                        else:\n",
    "                            llm_client = LocalLLMClient(model)    \n",
    "                        llm_client.prompt = request.prompt\n",
    "                        prompt = llm_client.generate_prompt(request.question, filtered_embeddings)\n",
    "                        num_tokens_prompt = len(llm_client.token_encoding.encode(prompt))\n",
    "                        #llm_client.priming = f'[INST]{llm_client.priming}[/INST] '\n",
    "                        num_tokens_priming = len(llm_client.token_encoding.encode(llm_client.priming))\n",
    "                        num_tokens = num_tokens_prompt + num_tokens_priming\n",
    "                        print(f'total tokens sent to LLM: {num_tokens}')\n",
    "                        #answer_generator = llm_client.get_completion(f'[INST]{prompt}[/INST]')\n",
    "                        answer_generator = llm_client.get_completion(prompt)\n",
    "\n",
    "                        running_tokens += num_tokens\n",
    "                        total_tokens += num_tokens\n",
    "                        time.sleep(delay)\n",
    "                        answer_text = \"\"\n",
    "                        for answer in answer_generator:\n",
    "                            answer_text += answer\n",
    "                        results[\"data\"].append({\n",
    "                            \"title\": title,\n",
    "                            \"id\": q_id,\n",
    "                            \"question\": question,\n",
    "                            \"answers\": answers,\n",
    "                            \"pred_answer\": answer_text,\n",
    "                            \"relevant_embeddings\": map_relevant_embeddings(filtered_embeddings)\n",
    "                        })\n",
    "                    # save results to file after each paragraph\n",
    "                    with open(RESULT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "                        print(f\"saved results to {RESULT_FILE} for title {title} paragraph {i}\")\n",
    "    print(f\"total tokens used: {total_tokens}\")"
   ],
   "id": "5a32e41e487e5db5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await run_all_qas(DATASET_FILE, model=Model.AZURE_GPT4)",
   "id": "8c0ada20fe3338e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'DATASET_FILE: {DATASET_FILE}')\n",
    "print(f'RESULT_FILE: {RESULT_FILE}')"
   ],
   "id": "fad1764c46183e9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from backend.evals.torchmetrics_eval_script import eval_squad_metrics\n",
    "\n",
    "eval_squad_metrics(RESULT_FILE)"
   ],
   "id": "b5da9105f24bd3e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# difference likely due to data cleaning (special char removal, lowercasing, etc.)\n",
    "from backend.evals.squad_eval_script import Evaluator\n",
    "import json\n",
    "\n",
    "evaluator = Evaluator(DATASET_FILE)\n",
    "em, f1, precision, recall = evaluator.evaluate(RESULT_FILE)\n",
    "print(f\"Exact Match: {em}\\n F1-measure: {f1}\\n Precision: {precision}\\n Recall: {recall}\")"
   ],
   "id": "f290323ac0d878da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "json_data = json.load(open(RESULT_FILE, \"r\", encoding=\"utf-8\"))\n",
    "for item in json_data[\"data\"]:\n",
    "    if len(item['relevant_embeddings']) > 1:\n",
    "        print(f\"question: {item['id']}\")"
   ],
   "id": "5a315eab41c37b5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from backend.evals.torchmetrics_eval_script import eval_retrieval_metrics\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "retrieval_metrics = []\n",
    "top_k=1\n",
    "for i in tqdm(range(1, top_k+1)):\n",
    "    retrieval_metrics.append(eval_retrieval_metrics(RESULT_FILE, top_k=i))\n",
    "\n",
    "for retrieval_metrics in retrieval_metrics:\n",
    "    print(f'{retrieval_metrics}')"
   ],
   "id": "4b6c147af96cc9a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAG evaluation",
   "id": "8102a5cdf6848a41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:58:02.189991Z",
     "start_time": "2024-07-08T07:58:02.180885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET = \"test\"\n",
    "DATASET_FILE = f\"../data/generated/{DATASET}.json\"\n",
    "#DATASET_FILE = f\"../data/{DATASET}.json\"\n",
    "RESULT_FILE = f\"../data/generated/results/{DATASET}_saul_reciprocal_cross_encoder.json\"\n",
    "#RESULT_FILE = f\"../data/generated/results/policyqa_{DATASET}_reciprocal_cross_encoder.json\"\n",
    "\n",
    "print(f'DATASET_FILE: {DATASET_FILE}')\n",
    "print(f'RESULT_FILE: {RESULT_FILE}')"
   ],
   "id": "2569b3e79c121d41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_FILE: ../data/generated/test.json\n",
      "RESULT_FILE: ../data/generated/results/test_saul_reciprocal_cross_encoder.json\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:58:18.703173Z",
     "start_time": "2024-07-08T07:58:10.874309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modules.llm.clients.openai.azure_openai_client import AzureOpenAILLMClient\n",
    "from modules.llm.clients.open_source.local_llm_client import LocalLLMClient\n",
    "from modules.llm.llm_infos import Model"
   ],
   "id": "74865eff07d570f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:01:59.151565Z",
     "start_time": "2024-07-08T08:01:59.139188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modules.answer.schemas import AnswerCreate\n",
    "from modules.answer.service import create as create_answer\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# switch rate limits for local and rest api\n",
    "#rate_limit_per_minute = 9600 # values from azure\n",
    "#tokens_per_minute = 40000 # values from azure\n",
    "rate_limit_per_minute = float(\"inf\") # local values\n",
    "tokens_per_minute = float(\"inf\") # local values\n",
    "delay = 60.0 / rate_limit_per_minute\n",
    "\n",
    "def get_saved_qas(file_path: str):\n",
    "    saved_qas = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "            if \"data\" in json_data:\n",
    "                for qa in json_data[\"data\"]:\n",
    "                    saved_qas.append(qa[\"id\"])\n",
    "    return saved_qas\n",
    "\n",
    "async def run_all_qas(file_path: str, model: Model=Model.AZURE_GPT4) -> None:\n",
    "    running_tokens = 0\n",
    "    total_tokens = 0\n",
    "    start_time = time.time()\n",
    "    results = {\"data\": []}\n",
    "\n",
    "    # check if there are saved results and load them\n",
    "    saved_qas = get_saved_qas(RESULT_FILE)\n",
    "    if len(saved_qas) > 0:\n",
    "        with open(RESULT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            results = json.load(f)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "        if \"data\" in json_data:\n",
    "            for regulation in tqdm(json_data[\"data\"]):\n",
    "                title = regulation[\"title\"]\n",
    "                logging.info(f\"processing regulation: {title}\")\n",
    "                for i, paragraph in tqdm(enumerate(regulation[\"paragraphs\"])):\n",
    "                    for qa in paragraph[\"qas\"]:\n",
    "                        q_id = qa[\"id\"]\n",
    "                        if q_id in saved_qas:\n",
    "                            print(f\"skipping question {q_id} because it is already saved\")\n",
    "                            continue\n",
    "                        question = qa[\"question\"]\n",
    "                        answers = qa[\"answers\"]\n",
    "                        # question_type = qa[\"type\"] # Only for policyqa dataset\n",
    "                        request = AnswerCreate(\n",
    "                            question=question,\n",
    "                            model=model,\n",
    "                            prompt=f\"\"\"You are a legal expert in ESG regulation, please give a concise answer to the question based on the given context. The answer must be an exact excerpt extracted from the given context that answers the question. Make sure to not add any additional or irrelevant information to the answer. Do not add quotes around the answer\"\"\"\n",
    "                            #prompt=f\"\"\"This is the PolicyQA dev dataset from the paper 'PolicyQA: A Reading Comprehension Dataset for Privacy Policies'. Answer the question by copying exactly a portion of the context shown after the delimiter ####.\n",
    "                            #The question category is specified in the following format practice|||attribute|||value and delimited by triple ticks, i.e ```. The question category is not part of the context.\n",
    "                            #The portion you copy can directly answer the question or be evidence that supports the answer. \n",
    "                            #The portion you copy can be any substring of the context, the whole context, or even a single word. But favor short and concise answers. \n",
    "                            #If you are not sure of the answer, copy the portion of the context that you think is relevant to the question.\n",
    "                            #Here is the question type: ```{question_type}```. Use the question type to understand the question and answer correctly.\n",
    "                            #Begin!\n",
    "                            ####\"\"\"\n",
    "                        )\n",
    "\n",
    "                        if running_tokens > tokens_per_minute*0.8 and time.time() - start_time < 60.0:\n",
    "                            time.sleep((time.time() - start_time)+15)\n",
    "                            start_time = time.time()\n",
    "                            running_tokens = 0\n",
    "\n",
    "                        question_embedding, answer_embeddings, answer_generator, num_tokens = await create_answer(request, title=title)\n",
    "\n",
    "                        running_tokens += num_tokens\n",
    "                        total_tokens += num_tokens\n",
    "                        \n",
    "                        time.sleep(delay)\n",
    "                        \n",
    "                        answer_text = \"\"\n",
    "                        for answer in answer_generator:\n",
    "                            answer_text += answer\n",
    "                            \n",
    "                        results[\"data\"].append({\n",
    "                            \"title\": title,\n",
    "                            \"id\": q_id,\n",
    "                            \"question\": question,\n",
    "                            \"answers\": answers,\n",
    "                            \"pred_answer\": answer_text,\n",
    "                            \"relevant_embeddings\": answer_embeddings\n",
    "                        })\n",
    "                    # save results to file after each paragraph\n",
    "                    with open(RESULT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "                        print(f\"saved results to {RESULT_FILE} for title {title} paragraph {i}\")\n",
    "    print(f\"total tokens used: {total_tokens}\")"
   ],
   "id": "3651cafc5b8786eb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await run_all_qas(DATASET_FILE, model=Model.Mistral)",
   "id": "e396fee62ede2e1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T12:23:00.527055Z",
     "start_time": "2024-07-08T12:22:54.587303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from backend.evals.torchmetrics_eval_script import eval_squad_metrics\n",
    "\n",
    "eval_squad_metrics(RESULT_FILE)"
   ],
   "id": "345012ce8eda7d17",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 263/263 [00:00<00:00, 275775.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': tensor(16.3498), 'f1': tensor(72.4641)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T12:23:03.527522Z",
     "start_time": "2024-07-08T12:23:03.483482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# difference likely due to data cleaning (special char removal, lowercasing, etc.)\n",
    "from backend.evals.squad_eval_script import Evaluator\n",
    "import json\n",
    "\n",
    "evaluator = Evaluator(DATASET_FILE)\n",
    "em, f1, precision, recall = evaluator.evaluate(RESULT_FILE)\n",
    "print(f\"Exact Match: {em}\\n F1-measure: {f1}\\n Precision: {precision}\\n Recall: {recall}\")"
   ],
   "id": "eedaaaa54402cfdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 14.828897338403042\n",
      " F1-measure: 72.89466640110831\n",
      " Precision: 76.3224921828521\n",
      " Recall: 79.12769965579824\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T12:23:09.291679Z",
     "start_time": "2024-07-08T12:23:06.589132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from backend.evals.torchmetrics_eval_script import eval_retrieval_metrics\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "retrieval_metrics = []\n",
    "top_k=3\n",
    "for i in tqdm(range(1, top_k+1)):\n",
    "    retrieval_metrics.append(eval_retrieval_metrics(RESULT_FILE, top_k=i))\n",
    "\n",
    "for retrieval_metrics in retrieval_metrics:\n",
    "    print(f'{retrieval_metrics}')"
   ],
   "id": "2fe4a3f09aaabf6c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "263it [00:00, 53637.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(question_ids): 263\n",
      "len(indexes): 263\n",
      "len(preds): 263\n",
      "len(targets): 263\n",
      "num_batches: 263\n",
      "len(indexes_batches): 263\n",
      "len(preds_batches): 263\n",
      "len(targets_batches): 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/263 [00:00<?, ?it/s]\u001B[A\n",
      " 13%|█▎        | 34/263 [00:00<00:00, 339.86it/s]\u001B[A\n",
      " 37%|███▋      | 98/263 [00:00<00:00, 511.47it/s]\u001B[A\n",
      " 57%|█████▋    | 150/263 [00:00<00:00, 437.02it/s]\u001B[A\n",
      " 74%|███████▍  | 195/263 [00:00<00:00, 383.60it/s]\u001B[A\n",
      "100%|██████████| 263/263 [00:01<00:00, 257.03it/s]\u001B[A\n",
      "C:\\Users\\onan\\IdeaProjects\\ai4esg_experiments_new\\backend\\evals\\torchmetrics_eval_script.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  relevant_embeddings_ratio = torch.tensor(sum(targets) / len(targets))\n",
      " 33%|███▎      | 1/3 [00:01<00:02,  1.08s/it]\n",
      "263it [00:00, 32874.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(question_ids): 263\n",
      "len(indexes): 526\n",
      "len(preds): 526\n",
      "len(targets): 526\n",
      "num_batches: 263\n",
      "len(indexes_batches): 263\n",
      "len(preds_batches): 263\n",
      "len(targets_batches): 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/263 [00:00<?, ?it/s]\u001B[A\n",
      " 31%|███       | 82/263 [00:00<00:00, 818.49it/s]\u001B[A\n",
      " 62%|██████▏   | 164/263 [00:00<00:00, 489.97it/s]\u001B[A\n",
      "100%|██████████| 263/263 [00:00<00:00, 353.72it/s]\u001B[A\n",
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.09it/s]\n",
      "263it [00:00, 18786.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(question_ids): 263\n",
      "len(indexes): 789\n",
      "len(preds): 789\n",
      "len(targets): 789\n",
      "num_batches: 263\n",
      "len(indexes_batches): 263\n",
      "len(preds_batches): 263\n",
      "len(targets_batches): 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/263 [00:00<?, ?it/s]\u001B[A\n",
      " 30%|██▉       | 78/263 [00:00<00:00, 779.84it/s]\u001B[A\n",
      " 59%|█████▉    | 156/263 [00:00<00:00, 509.83it/s]\u001B[A\n",
      " 81%|████████  | 213/263 [00:00<00:00, 388.49it/s]\u001B[A\n",
      "100%|██████████| 263/263 [00:00<00:00, 354.34it/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map@1': tensor(0.8479), 'mrr@1': tensor(0.8479), '%Answers found@1': tensor(0.8479), '%Relevant embeddings@1': tensor(0.8479)}\n",
      "{'map@2': tensor(0.8878), 'mrr@2': tensor(0.8878), '%Answers found@2': tensor(0.9278), '%Relevant embeddings@2': tensor(0.4734)}\n",
      "{'map@3': tensor(0.8945), 'mrr@3': tensor(0.8942), '%Answers found@3': tensor(0.9468), '%Relevant embeddings@3': tensor(0.3257)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "586dc88560e6c3d4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
