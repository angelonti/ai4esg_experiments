{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:57:26.038026Z",
     "start_time": "2024-10-10T16:57:26.028744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"../backend\")"
   ],
   "id": "8e7c9277d36ee993",
   "outputs": [],
   "execution_count": 257
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:57:26.347387Z",
     "start_time": "2024-10-10T16:57:26.343808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "not_matching = ['faf4435d-236c-4f96-b2e9-67a8f1c54a97',\n",
    "              '0edd772e-5cd5-468d-b43f-8c24619bb074',\n",
    "              'e74bffa4-eef0-4230-9607-7c11994e97cb',\n",
    "              'a05ed403-9305-4a50-b0d7-07995849d2d0',\n",
    "              '7fce4656-92ee-4c6f-85d4-6d13dabdbff6',\n",
    "              '80e19fa7-630a-4acf-8d4e-fd0d5cfd8b74',\n",
    "              '2b5d2ceb-5804-449a-b9b1-056a41839ad9',\n",
    "              'dbc90034-6cbe-492f-981f-ec046c250fad',\n",
    "              'aed5e05d-7315-4051-ab0b-6b73d5319420',\n",
    "              '13e3d2e8-80c6-4287-9d06-5adb7e115b75',\n",
    "              '28aeb085-d146-4499-b9ab-2f78e84255f3',\n",
    "              '5b1d10b3-81a4-45e0-8f10-78ade0a1c5a6',\n",
    "              '6a30bdea-935b-4229-acb4-004033f91f6a',\n",
    "              '0b7593c8-a51d-4181-9925-1c4923887da0',\n",
    "              'f8fc1e8e-fe69-44ec-9be3-5535c1fdbd54',\n",
    "              'f0943b3c-02af-4ec2-ae7b-b3b5e0012db3',\n",
    "              '6422a85e-362a-4e39-98b1-2b48897a5dbf',\n",
    "              'eadb1b89-9c1e-4c5c-82cf-c29a4c836701',\n",
    "              'e8a50210-8176-4fcc-9378-23cccb5da013',\n",
    "              'f7bbb89e-5215-4408-967d-85f7ea88a0bd',\n",
    "              '101743d7-6d0f-4802-8c9b-3ac58e1894c0',\n",
    "              'd2309370-7971-4a79-9b59-0f6b5bc7e9c3',\n",
    "              '9dedab11-341d-4e57-90af-c0f0911ebfcf',\n",
    "              '85d7e8fa-7bf6-42fe-9211-8f084dc45b53',\n",
    "              '8094bc64-a712-4307-a9f1-cfdf7c92648f',\n",
    "              '9f4efd09-4eb9-4b3b-b08c-87c7934807fd',\n",
    "              'f7e0b328-afce-49a2-ad52-4d587c125917',\n",
    "              'd9191db3-13dd-4d60-9bd9-00bc51c7df99',\n",
    "              '3a2f9205-cca8-42cb-b744-39e2c4a96155',\n",
    "              'd64ec2d4-ec3f-42d2-b2d4-e80a4bd95b6c',\n",
    "              '500fb50e-3d6a-4374-89ce-3430fad2e2ad',\n",
    "              '552922e3-9a90-46af-a25f-e5107db4a263',\n",
    "              '4c27f5d3-1830-4a9d-a0f1-63f2fad611ae',\n",
    "              '64763cbe-f956-4377-9691-9b19b54d606d',\n",
    "              'b5727112-2caf-4444-b85f-70e7e324d1f7',\n",
    "              '2563c8ee-74b8-4de4-80f7-5398d35f5edf']"
   ],
   "id": "dd5b4c563905932c",
   "outputs": [],
   "execution_count": 258
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:57:26.796919Z",
     "start_time": "2024-10-10T16:57:26.792717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET = \"train\"\n",
    "DATASET_FILE = f\"../data/generated/{DATASET}.json\"\n",
    "OUTPUT_FILE = f\"../data/generated/{DATASET}.tsv\""
   ],
   "id": "a1bf50d95d268021",
   "outputs": [],
   "execution_count": 259
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:57:27.192386Z",
     "start_time": "2024-10-10T16:57:27.188747Z"
    }
   },
   "cell_type": "code",
   "source": "print(DATASET_FILE)",
   "id": "8055bd5e1647c907",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/generated/train.json\n"
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-10T16:57:28.405453Z",
     "start_time": "2024-10-10T16:57:27.821092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "import tiktoken\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, LongformerTokenizer, T5Tokenizer, RobertaTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Ensure you have the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "token_encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def tokenize_long_text(text, max_length=512, stride=50):\n",
    "    tokens = []\n",
    "    for i in range(0, len(text), max_length - stride):\n",
    "        chunk = text[i:i + max_length]\n",
    "        tokenized_chunk = tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        tokens.extend(tokenized_chunk)\n",
    "    return tokens\n",
    "\n",
    "def create_tsv_from_json(json_file, tsv_file):\n",
    "    count_not_found = 0\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)['data']\n",
    "\n",
    "    with open(tsv_file, 'w', encoding='utf-8', newline='') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        writer.writerow(['question', 'tokenized_context', 'tag_annotations'])\n",
    "\n",
    "        for item in data:\n",
    "            for paragraph in item['paragraphs']:\n",
    "                context = paragraph['context'].replace('‘', ' ').replace('’', ' ').replace('\\n', ' ')\n",
    "                # tokenized_context = word_tokenize(context)\n",
    "                # tokenized_context = token_encoding.encode(context)\n",
    "                # tokenized_context = tokenize_long_text(context)\n",
    "                tokenized_context = tokenizer.encode(context, add_special_tokens=False)\n",
    "                for qa in paragraph['qas']:\n",
    "                    question = qa['question']\n",
    "                    id = qa['id']\n",
    "                    answer_text = qa['answers'][0]['text'].replace('‘', ' ').replace('’', ' ').replace('\\n', ' ')\n",
    "                    if id in not_matching:\n",
    "                        # set first character to lowercase\n",
    "                        answer_text = answer_text[0].lower() + answer_text[1:]\n",
    "                    # tokenized_answer = word_tokenize(answer_text)\n",
    "                    # tokenized_answer = token_encoding.encode(answer_text)\n",
    "                    # tokenized_answer = tokenize_long_text(answer_text)\n",
    "                    tokenized_answer = tokenizer.encode(answer_text, add_special_tokens=False)\n",
    "        \n",
    "                    # Create tag annotations\n",
    "                    found = False\n",
    "                    annotations = ['O'] * len(tokenized_context)\n",
    "                    for i in range(len(tokenized_context)):\n",
    "                        if tokenized_context[i:i + len(tokenized_answer)] == tokenized_answer:\n",
    "                            found = True\n",
    "                            annotations[i] = 'B'\n",
    "                            for j in range(1, len(tokenized_answer)):\n",
    "                                annotations[i + j] = 'I'\n",
    "                            break\n",
    "                    \n",
    "                    if found:\n",
    "                        #writer.writerow([question, ' '.join(tokenized_context), ' '.join(annotations)])\n",
    "                        writer.writerow([question, tokenized_context, annotations])\n",
    "                    else:\n",
    "                        print(f\"Answer not found in context for id: {id}\")\n",
    "                        count_not_found += 1\n",
    "\n",
    "    print(f\"Total answers not found in context: {count_not_found}\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\onan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 261
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T16:57:29.733153Z",
     "start_time": "2024-10-10T16:57:29.216910Z"
    }
   },
   "cell_type": "code",
   "source": "create_tsv_from_json(DATASET_FILE, OUTPUT_FILE)",
   "id": "d871767ecbeda173",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (901 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total answers not found in context: 0\n"
     ]
    }
   ],
   "execution_count": 262
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "451cddb215e9cda"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
